{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# engine\n",
    "\n",
    "> Simple engine that calculates gradients and backpropagates. Can be used in a simple neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from micrograd_nbdev.diagram import *\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *\n",
    "import torch\n",
    "import math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Value:\n",
    "    def __init__(self,\n",
    "                 data, # numeric value wrapped by `Value`\n",
    "                 _children=(), # inputs to a given `Value`\n",
    "                 _op='', # operation that resulted in a given `Value`\n",
    "                 label='' # label for plotting graphs\n",
    "                ):\n",
    "        self.data = data\n",
    "        self.grad = 0\n",
    "        # Internal variables for graphviz\n",
    "        self._backward = lambda : None\n",
    "        self._prev = set(_children)\n",
    "        self._op = _op\n",
    "        self.label=''\n",
    "    \n",
    "    @staticmethod\n",
    "    def _wrap(x):\n",
    "        \"Wrap x in Value\"\n",
    "        return x if isinstance(x, Value) else Value(x)\n",
    "        \n",
    "    def __add__(self, other):\n",
    "        \"self + other\"\n",
    "        other = self._wrap(other)\n",
    "        out = Value(self.data + other.data, (self, other), _op='+')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * 1 # global grad * local grad\n",
    "            other.grad += out.grad * 1\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def __radd__(self, other):\n",
    "        \"other + self\"\n",
    "        return self + other\n",
    "    \n",
    "    def __sub__(self, other):\n",
    "        \"self - other\"\n",
    "        return self + (-other)\n",
    "    \n",
    "    def __rsub__(self, other):\n",
    "        \"other - self\"\n",
    "        return other + (-self)\n",
    "    \n",
    "    def __mul__(self, other):\n",
    "        \"self * other\"\n",
    "        other = self._wrap(other)\n",
    "        out = Value(self.data * other.data, (self, other), _op='*')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other.data\n",
    "            other.grad += out.grad * self.data\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __rmul__(self, other):\n",
    "        \"other * mult\"\n",
    "        return self * other\n",
    " \n",
    "    def __pow__(self, other):\n",
    "        \"self ** other. Other should be int or float\"\n",
    "        assert isinstance(other, (int,float)), f\"{other} must be int or float\"\n",
    "        out = Value(self.data ** other, (self,), f\"**{other}\")\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * other * self.data**(other-1)\n",
    "            \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def __truediv__(self, other):\n",
    "        \"self / other\"\n",
    "        return self * other**-1\n",
    "    \n",
    "    def __rtruediv__(self, other):\n",
    "        \"other / self\"\n",
    "        return other * self**-1\n",
    "    \n",
    "    def __neg__(self):\n",
    "        \"-self\"\n",
    "        return self *-1\n",
    "    \n",
    "    def tanh(self):\n",
    "        \"tanh\"\n",
    "        out = Value(math.tanh(self.data), (self,) , 'tanh')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1-out**2)\n",
    "        \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "        \n",
    "    def relu(self):\n",
    "        \"relu\"\n",
    "        out = Value(max(self.data, 0), (self,), 'relu')\n",
    "        \n",
    "        def _backward():\n",
    "            self.grad += out.grad * (1 if self.data > 0 else 0)\n",
    "    \n",
    "        out._backward = _backward\n",
    "        return out\n",
    "    \n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "        \n",
    "        def topo_sort(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for child in node._prev:\n",
    "                    topo_sort(child)\n",
    "                    # ipdb.set_trace()\n",
    "                topo.append(node)\n",
    "        topo_sort(self)\n",
    "        \n",
    "        # print(f\"Topo list is {topo}\")\n",
    "        self.grad = 1\n",
    "        # need to reverse because it topo returns nodes from left-to-right \n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'Value(data={self.data})'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test simple operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(-4.0)\n",
    "b = Value(6.0)\n",
    "\n",
    "test_eq((a+b).data, -4+6)\n",
    "test_eq((a-b).data, -4-6)\n",
    "test_eq((a/b).data, -4/6)\n",
    "test_eq((a**2).data, (-4)**2)\n",
    "test_eq(a.relu().data, 0)\n",
    "test_eq(b.tanh().data, math.tanh(6.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test simple derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a*b\n",
    "c.grad = 1\n",
    "c._backward()\n",
    "test_eq(b.grad, a.data)\n",
    "test_eq(a.grad, b.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Value(4.0)\n",
    "b = a.relu()\n",
    "b.grad = 1\n",
    "b._backward()\n",
    "test_eq(a.grad, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-8.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Value(2.0, label='a') # 2\n",
    "b = Value(-3.0, label='b') # -3\n",
    "c = Value(10.0, label='c') # 10\n",
    "e = a*b; e.label = 'e' # -6\n",
    "d = e + c; d.label = 'd' # 4\n",
    "f = Value(-2.0, label='f') # -2\n",
    "L = d*f; L.label='L' # -8\n",
    "L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L.backward()\n",
    "#draw_dot(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Value(data=-20.0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Value(-4.0)\n",
    "z = 2 * x + 2 + x\n",
    "q = z.relu() + z * x\n",
    "h = (z * z).relu()\n",
    "y = h + q + q * x\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()\n",
    "#draw_dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test forward and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_1():\n",
    "\n",
    "    x = Value(-4.0)\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.relu() + z * x\n",
    "    h = (z * z).relu()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xmg, ymg = x, y\n",
    "\n",
    "    x = torch.Tensor([-4.0]).double()\n",
    "    x.requires_grad = True\n",
    "    z = 2 * x + 2 + x\n",
    "    q = z.relu() + z * x\n",
    "    h = (z * z).relu()\n",
    "    y = h + q + q * x\n",
    "    y.backward()\n",
    "    xpt, ypt = x, y\n",
    "\n",
    "    # forward pass went well\n",
    "    test_eq(ymg.data, ypt.data.item())\n",
    "    # backward pass went well\n",
    "    test_eq(xmg.grad, xpt.grad.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
