[
  {
    "objectID": "nn.html",
    "href": "nn.html",
    "title": "nn",
    "section": "",
    "text": "Neuron\n\nsource\n\n\nModule\n\n Module ()\n\nBase class\n\nsource\n\n\nNeuron\n\n Neuron (nin, nonlin=True)\n\nBase class\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnin\n\n\nnumber of inputs (parameters) for each node\n\n\nnonlin\nbool\nTrue\nwhether to use nonlinearity\n\n\n\n\nn = Neuron(5)\nn\n\nReLU Neuron(5)\n\n\n\nn.parameters()\n\n[Value(data=0.22203145250805423, grad=0),\n Value(data=0.8146914543486306, grad=0),\n Value(data=0.6845945253932413, grad=0),\n Value(data=-0.4951484752904616, grad=0),\n Value(data=0.5386153533408162, grad=0),\n Value(data=0, grad=0)]\n\n\n\nn([Value(1),Value(1),Value(1),Value(1),Value(1)])\n\nValue(data=1.7647843103002807, grad=0)\n\n\n\n\nLayer\n\nsource\n\n\nLayer\n\n Layer (nin, nout, **kwargs)\n\nBase class\n\nl = Layer(2, 1)\n\n\nl.parameters()\n\n[Value(data=0.5815738804807384, grad=0),\n Value(data=0.05953892107866232, grad=0),\n Value(data=0, grad=0)]\n\n\n\nl([Value(2.0), Value(1.0)])\n\nValue(data=1.2226866820401392, grad=0)\n\n\n\n\nMLP\n\nsource\n\n\nMLP\n\n MLP (nin, nouts)\n\nBase class\n\n\n\n\nDetails\n\n\n\n\nnin\nnumber of inputs\n\n\nnouts\nlist of inputs and outputs for each subsequent Layer\n\n\n\n\nmlp = MLP(2, [2,3,1]); mlp\n\nMLP of [Layer of [ReLU Neuron(2), ReLU Neuron(2)], Layer of [ReLU Neuron(2), ReLU Neuron(2), ReLU Neuron(2)], Layer of [Linear Neuron(3)]]\n\n\n\nmlp.parameters()\n\n[Value(data=-0.3131668656144113, grad=0),\n Value(data=0.3020218414831064, grad=0),\n Value(data=0, grad=0),\n Value(data=-0.6735470120050717, grad=0),\n Value(data=0.094031831508395, grad=0),\n Value(data=0, grad=0),\n Value(data=0.2564691606271521, grad=0),\n Value(data=0.7674418743222038, grad=0),\n Value(data=0, grad=0),\n Value(data=0.7161034265746886, grad=0),\n Value(data=0.9779788997553842, grad=0),\n Value(data=0, grad=0),\n Value(data=0.7679712408338863, grad=0),\n Value(data=0.4858063431729067, grad=0),\n Value(data=0, grad=0),\n Value(data=0.24332452137798088, grad=0),\n Value(data=-0.7837191969171422, grad=0),\n Value(data=-0.8833600660267253, grad=0),\n Value(data=0, grad=0)]"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "micrograd-nbdev",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "micrograd-nbdev",
    "section": "Install",
    "text": "Install\npip install micrograd_nbdev"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "micrograd-nbdev",
    "section": "How to use",
    "text": "How to use\nFill me in please! Donâ€™t forget code examples:\n\nnp.random.seed(1337)\nrandom.seed(1337)\n\n\n# make up a dataset\nfrom sklearn.datasets import make_moons, make_blobs\nX, y = make_moons(n_samples=100, noise=0.1)\n# make y -1 or 1\ny = y*2-1\n# visualize in 2D\nplt.figure(figsize=(5,5))\nplt.scatter(X[:,0], X[:,1], c=y, s=20, cmap='jet')\n\n<matplotlib.collections.PathCollection>\n\n\n\n\n\n\n# initialize a model with 2 features\nmodel = MLP(2,[16, 16, 1])\nprint(\"number of parameters\", len(model.parameters()))\n\nnumber of parameters 337\n\n\n\n# loss function\ndef loss(batch_size=None):\n    \n    # inline DataLoader :)\n    if batch_size is None:\n        Xb, yb = X, y\n    else:\n        ri = np.random.permutation(X.shape[0])[:batch_size]\n        Xb, yb = X[ri], y[ri]\n    inputs = [list(map(Value, xrow)) for xrow in Xb]\n    \n    # forward the model to get scores\n    scores = list(map(model, inputs))\n    \n    # svm \"max-margin\" loss\n    losses = [(1 + -yi*scorei).relu() for yi, scorei in zip(yb, scores)]\n    data_loss = sum(losses) * (1.0 / len(losses))\n    # L2 regularization\n    alpha = 1e-4\n    reg_loss = alpha * sum((p*p for p in model.parameters()))\n    total_loss = data_loss + reg_loss\n    \n    # also get accuracy\n    accuracy = [(yi > 0) == (scorei.data > 0) for yi, scorei in zip(yb, scores)]\n    return total_loss, sum(accuracy) / len(accuracy)\n\ntotal_loss, acc = loss()\nprint(total_loss, acc)\n\nValue(data=0.8958441028683222, grad=0) 0.5\n\n\n\n# optimization\nfor k in range(100):\n    # forward\n    total_loss, acc = loss()\n    \n    # backward\n    model.zero_grad()\n    total_loss.backward()\n    \n    # update (sgd)\n    learning_rate = 1.0 - 0.9*k/100\n    for p in model.parameters():\n        p.data -= learning_rate*p.grad\n    \n    if k%10 == 0:\n        print(f\"step: {k} | total_loss: {total_loss.data} | acc {acc*100}%\")\n\nstep: 0 | total_loss: 0.8958441028683222 | acc 50.0%\nstep: 10 | total_loss: 0.24507023853658036 | acc 91.0%\nstep: 20 | total_loss: 0.18977522856087645 | acc 91.0%\nstep: 30 | total_loss: 0.11731297569011845 | acc 95.0%\nstep: 40 | total_loss: 0.0601600989523447 | acc 100.0%\nstep: 50 | total_loss: 0.09875114765619608 | acc 96.0%\nstep: 60 | total_loss: 0.032597111578102286 | acc 99.0%\nstep: 70 | total_loss: 0.014229870065926908 | acc 100.0%\nstep: 80 | total_loss: 0.012318500800515816 | acc 100.0%\nstep: 90 | total_loss: 0.010984458327280174 | acc 100.0%"
  },
  {
    "objectID": "diagram.html",
    "href": "diagram.html",
    "title": "diagram",
    "section": "",
    "text": "source\n\ntrace\n\n trace (root)\n\n\n\n\n\nDetails\n\n\n\n\nroot\nValue node\n\n\n\n\na = Value(2.0, label='a') # 2\nb = Value(-3.0, label='b') # -3\nc = Value(10.0, label='c') # 10\ne = a*b; e.label = 'e' # -6\nd = e + c; d.label = 'd' # 4\nf = Value(-2.0, label='f') # -2\nL = d*f; L.label='L' # -8\nL\ntrace(L)\n\n({Value(data=-2.0, grad=0),\n  Value(data=-3.0, grad=0),\n  Value(data=-6.0, grad=0),\n  Value(data=-8.0, grad=0),\n  Value(data=10.0, grad=0),\n  Value(data=2.0, grad=0),\n  Value(data=4.0, grad=0)},\n {(Value(data=-6.0, grad=0), Value(data=-3.0, grad=0)),\n  (Value(data=-6.0, grad=0), Value(data=2.0, grad=0)),\n  (Value(data=-8.0, grad=0), Value(data=-2.0, grad=0)),\n  (Value(data=-8.0, grad=0), Value(data=4.0, grad=0)),\n  (Value(data=4.0, grad=0), Value(data=-6.0, grad=0)),\n  (Value(data=4.0, grad=0), Value(data=10.0, grad=0))})\n\n\n\nsource\n\n\ndraw_dot\n\n draw_dot (root, format='svg', rankdir='LR')\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nroot\n\n\nfinal node from which to trace\n\n\nformat\nstr\nsvg\nrendering output format of DOT\n\n\nrankdir\nstr\nLR\nTB (top to bottom graph) | LR (left to right)\n\n\n\n\n\nExamples\n\ndraw_dot(L)\n\n\n\n\n\nx = Value(-4.0)\nz = 2 * x + 2 + x\nq = z.relu() + z * x\nh = (z * z).relu()\ny = h + q + q * x\ny\ndraw_dot(y)"
  },
  {
    "objectID": "engine.html",
    "href": "engine.html",
    "title": "engine",
    "section": "",
    "text": "source\n\nValue\n\n Value (data, _children=(), _op='', label='')\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndata\n\n\nnumeric value wrapped by Value\n\n\n_children\ntuple\n()\ninputs to a given Value\n\n\n_op\nstr\n\noperation that resulted in a given Value\n\n\nlabel\nstr\n\nlabel for plotting graphs\n\n\n\n\n\nTest simple operations\n\na = Value(-4.0)\nb = Value(6.0)\n\ntest_eq((a+b).data, -4+6)\ntest_eq((a-b).data, -4-6)\ntest_eq((a/b).data, -4/6)\ntest_eq((a**2).data, (-4)**2)\ntest_eq(a.relu().data, 0)\ntest_eq(b.tanh().data, math.tanh(6.0))\n\n\n\nTest simple derivatives\n\nc = a*b\nc.grad = 1\nc._backward()\ntest_eq(b.grad, a.data)\ntest_eq(a.grad, b.data)\n\n\na = Value(4.0)\nb = a.relu()\nb.grad = 1\nb._backward()\ntest_eq(a.grad, 1)\n\n\na = Value(2.0, label='a') # 2\nb = Value(-3.0, label='b') # -3\nc = Value(10.0, label='c') # 10\ne = a*b; e.label = 'e' # -6\nd = e + c; d.label = 'd' # 4\nf = Value(-2.0, label='f') # -2\nL = d*f; L.label='L' # -8\nL\n\nValue(data=-8.0, grad=0)\n\n\n\n\nTest Backward\n\nL.backward()\ndraw_dot(L)\n\n\n\n\n\nx = Value(-4.0)\nz = 2 * x + 2 + x\nq = z.relu() + z * x\nh = (z * z).relu()\ny = h + q + q * x\ny\n\nValue(data=-20.0, grad=0)\n\n\n\ny.backward()\ndraw_dot(y)\n\n\n\n\n\n\nTest forward and backward pass\n\ndef test_1():\n\n    x = Value(-4.0)\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xmg, ymg = x, y\n\n    x = torch.Tensor([-4.0]).double()\n    x.requires_grad = True\n    z = 2 * x + 2 + x\n    q = z.relu() + z * x\n    h = (z * z).relu()\n    y = h + q + q * x\n    y.backward()\n    xpt, ypt = x, y\n\n    # forward pass went well\n    test_eq(ymg.data, ypt.data.item())\n    # backward pass went well\n    test_eq(xmg.grad, xpt.grad.item())\n\n\ndef test_2():\n\n    a = Value(-4.0)\n    b = Value(2.0)\n    c = a + b\n    d = a * b + b**3\n    c += c + 1\n    c += 1 + c + (-a)\n    d += d * 2 + (b + a).relu()\n    d += 3 * d + (b - a).relu()\n    e = c - d\n    f = e**2\n    g = f / 2.0\n    g += 10.0 / f\n    g.backward()\n    amg, bmg, gmg = a, b, g\n\n    a = torch.Tensor([-4.0]).double()\n    b = torch.Tensor([2.0]).double()\n    a.requires_grad = True\n    b.requires_grad = True\n    c = a + b\n    d = a * b + b**3\n    c = c + c + 1\n    c = c + 1 + c + (-a)\n    d = d + d * 2 + (b + a).relu()\n    d = d + 3 * d + (b - a).relu()\n    e = c - d\n    f = e**2\n    g = f / 2.0\n    g = g + 10.0 / f\n    g.backward()\n    apt, bpt, gpt = a, b, g\n\n    tol = 1e-6\n    # forward pass went well\n    assert abs(gmg.data - gpt.data.item()) < tol\n    # backward pass went well\n    assert abs(amg.grad - apt.grad.item()) < tol\n    assert abs(bmg.grad - bpt.grad.item()) < tol\n\n\ntest_1()\n\n\ntest_2()"
  }
]